{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMa5AWe77q7Fd80BtDXZk1M"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MSZxhpZoKq5y","executionInfo":{"status":"ok","timestamp":1725522964521,"user_tz":-540,"elapsed":15957,"user":{"displayName":"강충원","userId":"17445879083622129016"}},"outputId":"6ba69f64-86d0-413a-822d-c91230e59ccf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","import torch\n","import random\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n","from torch.optim import AdamW\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AlbertConfig, AutoConfig\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch.nn.functional as F\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0J87hxx1LA_h","executionInfo":{"status":"ok","timestamp":1725522971923,"user_tz":-540,"elapsed":6241,"user":{"displayName":"강충원","userId":"17445879083622129016"}},"outputId":"924c2c76-47ed-49db-90ff-a6975a9cd9ee"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","source":["class XLNet:\n","    def __init__(self, df, model_name=\"xlnet-base-cased\", max_length=64, learning_rate=2e-5, batch_size=16, epochs=10, patience=3, device=None):\n","        self.df = df\n","        self.model_name = model_name\n","        self.max_length = max_length\n","        self.learning_rate = learning_rate\n","        self.batch_size = batch_size\n","        self.epochs = epochs\n","        self.patience = patience\n","        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        # Tokenizer와 Config 불러오기\n","        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n","        self.config = AutoConfig.from_pretrained(self.model_name, hidden_dropout_prob=0.2, num_labels=1)  # num_labels=1로 설정\n","        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, config=self.config)\n","\n","        # 분류기 레이어를 이진 분류에 맞게 수정\n","        self.model.classifier = torch.nn.Linear(self.model.config.hidden_size, 1)\n","\n","        self.optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n","        self.best_model_state_dict = None\n","        self.best_accuracy = 0\n","\n","    @staticmethod\n","    def set_seed(seed):\n","        random.seed(seed)\n","        np.random.seed(seed)\n","        torch.manual_seed(seed)\n","        if torch.cuda.is_available():\n","            torch.cuda.manual_seed_all(seed)\n","\n","    def preprocess_data(self, df):\n","        inputs = self.tokenizer(\n","            list(df['prepro']),\n","            padding=True,\n","            truncation=True,\n","            max_length=self.max_length,\n","            return_tensors=\"pt\"\n","        )\n","        labels = torch.tensor(df['label'].values)\n","        return DataLoader(TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels), batch_size=self.batch_size, shuffle=True)\n","\n","    def train(self):\n","      df_train, df_temp = train_test_split(self.df, test_size=0.4, random_state=42)\n","      df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n","      train_loader = self.preprocess_data(df_train)\n","      val_loader = self.preprocess_data(df_val)\n","\n","      self.model.to(self.device)\n","\n","      patience_counter = 0\n","      min_val_loss = float('inf')\n","\n","      try:\n","          for epoch in range(self.epochs):\n","              print(f\"\\nEpoch {epoch + 1}/{self.epochs}\")\n","              self.model.train()\n","\n","              for input_batch in tqdm(train_loader, desc=\"Training Batches\", leave=False):\n","                  input_ids, attention_mask, label_batch = [tensor.to(self.device) for tensor in input_batch]\n","\n","                  self.optimizer.zero_grad()\n","                  outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n","\n","                  logits = outputs.logits.squeeze(dim=-1)\n","                  loss = F.binary_cross_entropy_with_logits(logits, label_batch.float())\n","\n","                  loss.backward()\n","                  self.optimizer.step()\n","\n","              # Validation 단계\n","              self.model.eval()\n","              val_loss_total = 0\n","              val_predictions_all = []\n","              val_labels_all = []\n","\n","              with torch.inference_mode():\n","                  for val_batch in tqdm(val_loader, desc=\"Validation Batches\", leave=False):\n","                      input_ids, attention_mask, val_labels = [tensor.to(self.device) for tensor in val_batch]\n","\n","                      val_outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n","                      val_logits = val_outputs.logits.squeeze(dim=-1)\n","\n","                      val_predictions = torch.round(torch.sigmoid(val_logits))\n","                      val_loss = F.binary_cross_entropy_with_logits(val_logits, val_labels.float())\n","                      val_loss_total += val_loss.item()\n","\n","                      val_predictions_all.append(val_predictions.cpu())\n","                      val_labels_all.append(val_labels.cpu())\n","\n","              val_predictions_all = torch.cat(val_predictions_all)\n","              val_labels_all = torch.cat(val_labels_all)\n","\n","              val_accuracy = accuracy_score(val_labels_all, val_predictions_all)\n","              val_f1 = f1_score(val_labels_all, val_predictions_all)\n","              val_recall = recall_score(val_labels_all, val_predictions_all)\n","              val_precision = precision_score(val_labels_all, val_predictions_all)\n","\n","              val_loss_total /= len(val_loader)\n","              print(f'\\nValidation Loss: {val_loss_total:.4f}, Accuracy: {val_accuracy:.4f}, F1: {val_f1:.4f}, Recall: {val_recall:.4f}, Precision: {val_precision:.4f}')\n","\n","              if val_loss_total < min_val_loss:\n","                  min_val_loss = val_loss_total\n","                  patience_counter = 0\n","                  self.best_model_state_dict = self.model.state_dict().copy()\n","              else:\n","                  patience_counter += 1\n","\n","              if patience_counter >= self.patience:\n","                print(f\"Early stopping at epoch {epoch + 1}\")\n","                # 가장 좋은 모델 상태를 파일로 저장\n","                torch.save(self.best_model_state_dict, \"best_model(XLNet)_checkpoint.pth\")\n","                break\n","\n","      except Exception as e:\n","          print(f\"An error occurred during training: {str(e)}\")\n","          print(\"Saving current model weights...\")\n","          # 현재까지의 가중치를 저장\n","          torch.save(self.model.state_dict(), \"error_model(XLNet).pth\")\n","          raise  # 오류를 다시 발생시켜서 코드 실행 중단\n","\n","    def evaluate(self):\n","      if self.best_model_state_dict is None:\n","          raise ValueError(\"No trained model found. Please train the model first.\")\n","\n","      # 테스트 데이터 준비\n","      df_train, df_temp = train_test_split(self.df, test_size=0.4, random_state=42)\n","      df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n","      test_loader = self.preprocess_data(df_test)\n","\n","      # 저장된 최상의 모델 로드\n","      best_model = AutoModelForSequenceClassification.from_pretrained(self.model_name, config=self.config)\n","      best_model.load_state_dict(self.best_model_state_dict, strict=False)  # strict=False로 로드\n","      best_model.to(self.device)\n","\n","      best_model.eval()\n","      test_predictions_all = []\n","      test_labels_all = []\n","\n","      try:\n","          with torch.inference_mode():\n","              for test_batch in tqdm(test_loader, desc=\"Test Batches\", leave=False):\n","                  input_ids, attention_mask, test_labels = [tensor.to(self.device) for tensor in test_batch]\n","\n","                  test_outputs = best_model(input_ids=input_ids, attention_mask=attention_mask)\n","                  test_logits = test_outputs.logits.squeeze(dim=-1)\n","\n","                  test_predictions = torch.round(torch.sigmoid(test_logits))\n","\n","                  test_predictions_all.append(test_predictions.cpu())\n","                  test_labels_all.append(test_labels.cpu())\n","\n","          test_predictions_all = torch.cat(test_predictions_all)\n","          test_labels_all = torch.cat(test_labels_all)\n","\n","          accuracy = accuracy_score(test_labels_all, test_predictions_all)\n","          f1 = f1_score(test_labels_all, test_predictions_all)\n","          recall = recall_score(test_labels_all, test_predictions_all)\n","          precision = precision_score(test_labels_all, test_predictions_all)\n","\n","          print(f'Test Accuracy: {accuracy:.4f}')\n","          print(f'Test F1 Score: {f1:.4f}')\n","          print(f'Test Recall: {recall:.4f}')\n","          print(f'Test Precision: {precision:.4f}')\n","\n","          # 모델 저장\n","          torch.save(best_model.state_dict(), \"best_model(XLNet).pth\")\n","\n","      except Exception as e:\n","          print(f\"An error occurred during evaluation: {str(e)}\")\n","          print(\"Saving current model weights...\")\n","          torch.save(best_model.state_dict(), \"error_model(XLNet_evaluation).pth\")\n","          raise\n","\n","      try:\n","          with torch.inference_mode():\n","              for test_batch in tqdm(test_loader, desc=\"Test Batches\", leave=False):\n","                  input_ids, attention_mask, test_labels = [tensor.to(self.device) for tensor in test_batch]\n","\n","                  test_outputs = best_model(input_ids=input_ids, attention_mask=attention_mask)\n","                  test_logits = test_outputs.logits.squeeze(dim=-1)\n","\n","                  test_predictions = torch.round(torch.sigmoid(test_logits))\n","\n","                  test_predictions_all.append(test_predictions.cpu())\n","                  test_labels_all.append(test_labels.cpu())\n","\n","          test_predictions_all = torch.cat(test_predictions_all)\n","          test_labels_all = torch.cat(test_labels_all)\n","\n","          accuracy = accuracy_score(test_labels_all, test_predictions_all)\n","          f1 = f1_score(test_labels_all, test_predictions_all)\n","          recall = recall_score(test_labels_all, test_predictions_all)\n","          precision = precision_score(test_labels_all, test_predictions_all)\n","\n","          print(f'Test Accuracy: {accuracy:.4f}')\n","          print(f'Test F1 Score: {f1:.4f}')\n","          print(f'Test Recall: {recall:.4f}')\n","          print(f'Test Precision: {precision:.4f}')\n","\n","          # 모델 저장\n","          torch.save(best_model.state_dict(), \"best_model(XLNet).pth\")\n","\n","      except Exception as e:\n","          print(f\"An error occurred during evaluation: {str(e)}\")\n","          print(\"Saving current model weights...\")\n","          # 현재까지의 가중치를 저장\n","          torch.save(best_model.state_dict(), \"error_model(XLNet_evaluation).pth\")\n","          raise  # 오류를 다시 발생시켜서 코드 실행 중단\n","\n","def main():\n","    df = pd.read_csv(\"/content/drive/MyDrive/IMCOM_Edtech_apps(prepro+sentiment).csv\")\n","    classifier = XLNet(df)\n","    classifier.set_seed(42)\n","    classifier.train()\n","    classifier.evaluate()\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":962},"id":"2n3Os4m8LDSB","executionInfo":{"status":"error","timestamp":1725529632192,"user_tz":-540,"elapsed":1055538,"user":{"displayName":"강충원","userId":"17445879083622129016"}},"outputId":"fc23716a-435b-4147-9c03-045cb3b99503"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1/10\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Validation Loss: 0.3086, Accuracy: 0.8935, F1: 0.8891, Recall: 0.9059, Precision: 0.8730\n","\n","Epoch 2/10\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Validation Loss: 0.2685, Accuracy: 0.8996, F1: 0.8935, Recall: 0.8936, Precision: 0.8934\n","\n","Epoch 3/10\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Validation Loss: 0.2836, Accuracy: 0.9006, F1: 0.8925, Recall: 0.8752, Precision: 0.9105\n","\n","Epoch 4/10\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Validation Loss: 0.3133, Accuracy: 0.8961, F1: 0.8876, Recall: 0.8705, Precision: 0.9053\n","\n","Epoch 5/10\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Validation Loss: 0.3094, Accuracy: 0.8927, F1: 0.8895, Recall: 0.9166, Precision: 0.8640\n","Early stopping at epoch 5\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.8905\n","Test F1 Score: 0.8856\n","Test Recall: 0.9104\n","Test Precision: 0.8620\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["An error occurred during evaluation: 'Tensor' object has no attribute 'append'\n","Saving current model weights...\n"]},{"output_type":"error","ename":"AttributeError","evalue":"'Tensor' object has no attribute 'append'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-4ab48020fac1>\u001b[0m in \u001b[0;36m<cell line: 218>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-16-4ab48020fac1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-4ab48020fac1>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m                   \u001b[0mtest_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                   \u001b[0mtest_predictions_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m                   \u001b[0mtest_labels_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'append'"]}]}]}