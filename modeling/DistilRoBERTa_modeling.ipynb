{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from torch.optim import AdamW \n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AlbertConfig, AutoConfig\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distilroberta:\n",
    "    def __init__(self, df, model_name='distilbert/distilroberta-base', max_length=64, learning_rate=2e-5, batch_size=16, epochs=10, patience=3, device=None):\n",
    "        self.df = df\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.patience = patience\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Tokenizer와 ALBERT 전용 Config 불러오기\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.config = AutoConfig.from_pretrained(self.model_name, hidden_dropout_prob=0.2, num_labels=1)  # num_labels=1로 설정\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, config=self.config, ignore_mismatched_sizes=True)\n",
    "        \n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.best_model_state_dict = None\n",
    "        self.best_accuracy = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def set_seed(seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    def preprocess_data(self, df):\n",
    "        inputs = self.tokenizer(\n",
    "            list(df['prepro']),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = torch.tensor(df['label'].values)\n",
    "        return DataLoader(TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels), batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def train(self):\n",
    "        df_train, df_temp = train_test_split(self.df, test_size=0.4, random_state=42)\n",
    "        df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
    "        train_loader = self.preprocess_data(df_train)\n",
    "        val_loader = self.preprocess_data(df_val)\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        patience_counter = 0\n",
    "        min_val_loss = float('inf')\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{self.epochs}\")\n",
    "            self.model.train()\n",
    "\n",
    "            for input_batch in tqdm(train_loader, desc=\"Training Batches\", leave=False):\n",
    "                input_ids, attention_mask, label_batch = [tensor.to(self.device) for tensor in input_batch]\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                \n",
    "                # Logits 크기 및 타겟 크기 확인\n",
    "                #print(f\"Logits size: {outputs.logits.size()}, Labels size: {label_batch.size()}\")  # 로깅\n",
    "                \n",
    "                # 이진 분류의 경우 logits 차원 축소\n",
    "                logits = outputs.logits.squeeze(dim=-1)\n",
    "                #print(f\"Logits after squeeze: {logits.size()}\")  # Log the logits after squeezing\n",
    "                \n",
    "                # loss 계산 시 레이블을 float으로 변환\n",
    "                loss = F.binary_cross_entropy_with_logits(logits, label_batch.float())  # Binary classification\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # Validation 단계\n",
    "            self.model.eval()\n",
    "            val_loss_total = 0\n",
    "            val_predictions_all = []\n",
    "            val_labels_all = []\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                for val_batch in tqdm(val_loader, desc=\"Validation Batches\", leave=False):\n",
    "                    input_ids, attention_mask, val_labels = [tensor.to(self.device) for tensor in val_batch]\n",
    "\n",
    "                    val_outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    val_logits = val_outputs.logits.squeeze(dim=-1)\n",
    "\n",
    "                    # Binary classification: use sigmoid and round for predictions\n",
    "                    val_predictions = torch.round(torch.sigmoid(val_logits))\n",
    "\n",
    "                    # Validation loss 계산\n",
    "                    val_loss = F.binary_cross_entropy_with_logits(val_logits, val_labels.float())\n",
    "                    val_loss_total += val_loss.item()\n",
    "\n",
    "                    val_predictions_all.append(val_predictions.cpu())\n",
    "                    val_labels_all.append(val_labels.cpu())\n",
    "\n",
    "            # 리스트들을 연결\n",
    "            val_predictions_all = torch.cat(val_predictions_all)\n",
    "            val_labels_all = torch.cat(val_labels_all)\n",
    "\n",
    "            # Validation 지표 계산\n",
    "            val_accuracy = accuracy_score(val_labels_all, val_predictions_all)\n",
    "            val_f1 = f1_score(val_labels_all, val_predictions_all)\n",
    "            val_recall = recall_score(val_labels_all, val_predictions_all)\n",
    "            val_precision = precision_score(val_labels_all, val_predictions_all)\n",
    "\n",
    "            val_loss_total /= len(val_loader)\n",
    "            print(f'\\nValidation Loss: {val_loss_total:.4f}, Accuracy: {val_accuracy:.4f}, F1: {val_f1:.4f}, Recall: {val_recall:.4f}, Precision: {val_precision:.4f}')\n",
    "\n",
    "            # Early stopping logic\n",
    "            if val_loss_total < min_val_loss:\n",
    "                min_val_loss = val_loss_total\n",
    "                patience_counter = 0\n",
    "                self.best_model_state_dict = self.model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= self.patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "    def evaluate(self):\n",
    "        if self.best_model_state_dict is None:\n",
    "            raise ValueError(\"No trained model found. Please train the model first.\")\n",
    "\n",
    "        df_train, df_temp = train_test_split(self.df, test_size=0.4, random_state=42)\n",
    "        df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
    "        test_loader = self.preprocess_data(df_test)\n",
    "\n",
    "        best_model = AutoModelForSequenceClassification.from_pretrained(self.model_name, config=self.config)\n",
    "        best_model.load_state_dict(self.best_model_state_dict)\n",
    "        best_model.to(self.device)\n",
    "\n",
    "        best_model.eval()\n",
    "        test_predictions_all = []\n",
    "        test_labels_all = []\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            for test_batch in tqdm(test_loader, desc=\"Test Batches\", leave=False):\n",
    "                input_ids, attention_mask, test_labels = [tensor.to(self.device) for tensor in test_batch]\n",
    "\n",
    "                test_outputs = best_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                test_logits = test_outputs.logits.squeeze(dim=-1)\n",
    "\n",
    "                # Binary classification: use sigmoid and round for predictions\n",
    "                test_predictions = torch.round(torch.sigmoid(test_logits))\n",
    "\n",
    "                test_predictions_all.append(test_predictions.cpu())\n",
    "                test_labels_all.append(test_labels.cpu())\n",
    "\n",
    "        # 리스트들을 연결\n",
    "        test_predictions_all = torch.cat(test_predictions_all)\n",
    "        test_labels_all = torch.cat(test_labels_all)\n",
    "\n",
    "        # 평가 지표 계산\n",
    "        accuracy = accuracy_score(test_labels_all, test_predictions_all)\n",
    "        f1 = f1_score(test_labels_all, test_predictions_all)\n",
    "        recall = recall_score(test_labels_all, test_predictions_all)\n",
    "        precision = precision_score(test_labels_all, test_predictions_all)\n",
    "\n",
    "        print(f'Test Accuracy: {accuracy:.4f}')\n",
    "        print(f'Test F1 Score: {f1:.4f}')\n",
    "        print(f'Test Recall: {recall:.4f}')\n",
    "        print(f'Test Precision: {precision:.4f}')\n",
    "\n",
    "        # 모델 저장\n",
    "        torch.save(best_model.state_dict(), \"best_model(DistilRoBERTa).pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub\\models--distilbert--distilroberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilbert/distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.2922, Accuracy: 0.8971, F1: 0.8905, Recall: 0.8879, Precision: 0.8930\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.2703, Accuracy: 0.9006, F1: 0.8956, Recall: 0.9040, Precision: 0.8873\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.2816, Accuracy: 0.8978, F1: 0.8885, Recall: 0.8643, Precision: 0.9142\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.3055, Accuracy: 0.8988, F1: 0.8898, Recall: 0.8667, Precision: 0.9141\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.2907, Accuracy: 0.8976, F1: 0.8934, Recall: 0.9112, Precision: 0.8764\n",
      "Early stopping at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilbert/distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8972\n",
      "Test F1 Score: 0.8914\n",
      "Test Recall: 0.9071\n",
      "Test Precision: 0.8763\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    df = pd.read_csv(\"C:\\\\Users\\\\USER\\\\Desktop\\\\충원's project\\\\IMCOM\\\\IMCOM_Edtech_apps(prepro+sentiment).csv\")\n",
    "    classifier = Distilroberta(df)\n",
    "    classifier.set_seed(42)\n",
    "    classifier.train()\n",
    "    classifier.evaluate()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "class Distilroberta:\n",
    "    def __init__(self, df, model_name='distilbert/distilroberta-base', max_length=64, batch_size=16, device=None):\n",
    "        self.df = df\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Tokenizer와 모델 초기화\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.config = AutoConfig.from_pretrained(self.model_name, num_labels=1)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, config=self.config)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def preprocess_data(self, df):\n",
    "        inputs = self.tokenizer(\n",
    "            list(df['prepro']),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = torch.tensor(df['label'].values)\n",
    "        return DataLoader(TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels), batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        # 저장된 가중치 로드\n",
    "        self.model.load_state_dict(torch.load(path, map_location=self.device))\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def evaluate(self):\n",
    "        df_train, df_temp = train_test_split(self.df, test_size=0.4, random_state=42)\n",
    "        df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
    "        test_loader = self.preprocess_data(df_test)\n",
    "\n",
    "        self.model.eval()  # 평가 모드로 전환\n",
    "        test_predictions_all = []\n",
    "        test_labels_all = []\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            for test_batch in tqdm(test_loader, desc=\"Test Batches\", leave=False):\n",
    "                input_ids, attention_mask, test_labels = [tensor.to(self.device) for tensor in test_batch]\n",
    "\n",
    "                test_outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                test_logits = test_outputs.logits.squeeze(dim=-1)\n",
    "\n",
    "                # Binary classification: use sigmoid and round for predictions\n",
    "                test_predictions = torch.round(torch.sigmoid(test_logits))\n",
    "\n",
    "                test_predictions_all.append(test_predictions.cpu())\n",
    "                test_labels_all.append(test_labels.cpu())\n",
    "\n",
    "        # 리스트들을 연결\n",
    "        test_predictions_all = torch.cat(test_predictions_all)\n",
    "        test_labels_all = torch.cat(test_labels_all)\n",
    "\n",
    "        # 평가 지표 계산\n",
    "        accuracy = accuracy_score(test_labels_all, test_predictions_all)\n",
    "        f1 = f1_score(test_labels_all, test_predictions_all)\n",
    "        recall = recall_score(test_labels_all, test_predictions_all)\n",
    "        precision = precision_score(test_labels_all, test_predictions_all)\n",
    "\n",
    "        # 평가 결과 출력\n",
    "        print(f'Test Accuracy: {accuracy:.4f}')\n",
    "        print(f'Test F1 Score: {f1:.4f}')\n",
    "        print(f'Test Recall: {recall:.4f}')\n",
    "        print(f'Test Precision: {precision:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilbert/distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_23148\\769927200.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(path, map_location=self.device))\n",
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8972\n",
      "Test F1 Score: 0.8914\n",
      "Test Recall: 0.9071\n",
      "Test Precision: 0.8763\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    df = pd.read_csv(\"C:\\\\Users\\\\USER\\\\Desktop\\\\충원's project\\\\IMCOM\\\\IMCOM_Edtech_apps(prepro+sentiment).csv\")\n",
    "    classifier = Distilroberta(df)\n",
    "    classifier.load_weights(\"best_model(DistilRoBERTa).pth\")  # 저장된 가중치 로드\n",
    "    classifier.evaluate()  # 로드된 가중치로 Test 데이터 평가\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
