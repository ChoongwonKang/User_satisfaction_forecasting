{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Using cached huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.7.24-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.4-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Using cached tokenizers-0.19.1-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Using cached transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "Using cached huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n",
      "Using cached regex-2024.7.24-cp311-cp311-win_amd64.whl (269 kB)\n",
      "Using cached safetensors-0.4.4-cp311-none-win_amd64.whl (285 kB)\n",
      "Using cached tokenizers-0.19.1-cp311-none-win_amd64.whl (2.2 MB)\n",
      "Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, fsspec, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed fsspec-2024.6.1 huggingface-hub-0.24.6 regex-2024.7.24 safetensors-0.4.4 tokenizers-0.19.1 tqdm-4.66.5 transformers-4.44.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from torch.optim import AdamW \n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AlbertConfig, AutoConfig\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier:\n",
    "    def __init__(self, df, model_name=\"albert-base-v2\", max_length=64, learning_rate=2e-5, batch_size=16, epochs=10, patience=3, device=None):\n",
    "        self.df = df\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.patience = patience\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Tokenizer와 ALBERT 전용 Config 불러오기\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.config = AutoConfig.from_pretrained(self.model_name, hidden_dropout_prob=0.2, num_labels=1)  # num_labels=1로 설정\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, config=self.config, ignore_mismatched_sizes=True)\n",
    "        \n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.best_model_state_dict = None\n",
    "        self.best_accuracy = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def set_seed(seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    def preprocess_data(self, df):\n",
    "        inputs = self.tokenizer(\n",
    "            list(df['prepro']),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = torch.tensor(df['label'].values)\n",
    "        return DataLoader(TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels), batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def train(self):\n",
    "        df_train, df_temp = train_test_split(self.df, test_size=0.4, random_state=42)\n",
    "        df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
    "        train_loader = self.preprocess_data(df_train)\n",
    "        val_loader = self.preprocess_data(df_val)\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        patience_counter = 0\n",
    "        min_val_loss = float('inf')\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{self.epochs}\")\n",
    "            self.model.train()\n",
    "\n",
    "            for input_batch in tqdm(train_loader, desc=\"Training Batches\", leave=False):\n",
    "                input_ids, attention_mask, label_batch = [tensor.to(self.device) for tensor in input_batch]\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                \n",
    "                # Logits 크기 및 타겟 크기 확인\n",
    "                #print(f\"Logits size: {outputs.logits.size()}, Labels size: {label_batch.size()}\")  # 로깅\n",
    "                \n",
    "                # 이진 분류의 경우 logits 차원 축소\n",
    "                logits = outputs.logits.squeeze(dim=-1)\n",
    "                #print(f\"Logits after squeeze: {logits.size()}\")  # Log the logits after squeezing\n",
    "                \n",
    "                # loss 계산 시 레이블을 float으로 변환\n",
    "                loss = F.binary_cross_entropy_with_logits(logits, label_batch.float())  # Binary classification\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # Validation 단계\n",
    "            self.model.eval()\n",
    "            val_loss_total = 0\n",
    "            val_predictions_all = []\n",
    "            val_labels_all = []\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                for val_batch in tqdm(val_loader, desc=\"Validation Batches\", leave=False):\n",
    "                    input_ids, attention_mask, val_labels = [tensor.to(self.device) for tensor in val_batch]\n",
    "\n",
    "                    val_outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    val_logits = val_outputs.logits.squeeze(dim=-1)\n",
    "\n",
    "                    # Binary classification: use sigmoid and round for predictions\n",
    "                    val_predictions = torch.round(torch.sigmoid(val_logits))\n",
    "\n",
    "                    # Validation loss 계산\n",
    "                    val_loss = F.binary_cross_entropy_with_logits(val_logits, val_labels.float())\n",
    "                    val_loss_total += val_loss.item()\n",
    "\n",
    "                    val_predictions_all.append(val_predictions.cpu())\n",
    "                    val_labels_all.append(val_labels.cpu())\n",
    "\n",
    "            # 리스트들을 연결\n",
    "            val_predictions_all = torch.cat(val_predictions_all)\n",
    "            val_labels_all = torch.cat(val_labels_all)\n",
    "\n",
    "            # Validation 지표 계산\n",
    "            val_accuracy = accuracy_score(val_labels_all, val_predictions_all)\n",
    "            val_f1 = f1_score(val_labels_all, val_predictions_all)\n",
    "            val_recall = recall_score(val_labels_all, val_predictions_all)\n",
    "            val_precision = precision_score(val_labels_all, val_predictions_all)\n",
    "\n",
    "            val_loss_total /= len(val_loader)\n",
    "            print(f'\\nValidation Loss: {val_loss_total:.4f}, Accuracy: {val_accuracy:.4f}, F1: {val_f1:.4f}, Recall: {val_recall:.4f}, Precision: {val_precision:.4f}')\n",
    "\n",
    "            # Early stopping logic\n",
    "            if val_loss_total < min_val_loss:\n",
    "                min_val_loss = val_loss_total\n",
    "                patience_counter = 0\n",
    "                self.best_model_state_dict = self.model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= self.patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "    def evaluate(self):\n",
    "        if self.best_model_state_dict is None:\n",
    "            raise ValueError(\"No trained model found. Please train the model first.\")\n",
    "\n",
    "        df_train, df_temp = train_test_split(self.df, test_size=0.4, random_state=42)\n",
    "        df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
    "        test_loader = self.preprocess_data(df_test)\n",
    "\n",
    "        best_model = AutoModelForSequenceClassification.from_pretrained(self.model_name, config=self.config)\n",
    "        best_model.load_state_dict(self.best_model_state_dict)\n",
    "        best_model.to(self.device)\n",
    "\n",
    "        best_model.eval()\n",
    "        test_predictions_all = []\n",
    "        test_labels_all = []\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            for test_batch in tqdm(test_loader, desc=\"Test Batches\", leave=False):\n",
    "                input_ids, attention_mask, test_labels = [tensor.to(self.device) for tensor in test_batch]\n",
    "\n",
    "                test_outputs = best_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                test_logits = test_outputs.logits.squeeze(dim=-1)\n",
    "\n",
    "                # Binary classification: use sigmoid and round for predictions\n",
    "                test_predictions = torch.round(torch.sigmoid(test_logits))\n",
    "\n",
    "                test_predictions_all.append(test_predictions.cpu())\n",
    "                test_labels_all.append(test_labels.cpu())\n",
    "\n",
    "        # 리스트들을 연결\n",
    "        test_predictions_all = torch.cat(test_predictions_all)\n",
    "        test_labels_all = torch.cat(test_labels_all)\n",
    "\n",
    "        # 평가 지표 계산\n",
    "        accuracy = accuracy_score(test_labels_all, test_predictions_all)\n",
    "        f1 = f1_score(test_labels_all, test_predictions_all)\n",
    "        recall = recall_score(test_labels_all, test_predictions_all)\n",
    "        precision = precision_score(test_labels_all, test_predictions_all)\n",
    "\n",
    "        print(f'Test Accuracy: {accuracy:.4f}')\n",
    "        print(f'Test F1 Score: {f1:.4f}')\n",
    "        print(f'Test Recall: {recall:.4f}')\n",
    "        print(f'Test Precision: {precision:.4f}')\n",
    "\n",
    "        # 모델 저장\n",
    "        torch.save(best_model.state_dict(), \"best_model(ALBERT_sentiment2).pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.3167, Accuracy: 0.8876, F1: 0.8808, Recall: 0.8809, Precision: 0.8807\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.2760, Accuracy: 0.8936, F1: 0.8906, Recall: 0.9193, Precision: 0.8638\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.2840, Accuracy: 0.8917, F1: 0.8806, Recall: 0.8474, Precision: 0.9166\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.2873, Accuracy: 0.8974, F1: 0.8932, Recall: 0.9101, Precision: 0.8769\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.2652, Accuracy: 0.9028, F1: 0.8964, Recall: 0.8923, Precision: 0.9006\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.2740, Accuracy: 0.8964, F1: 0.8860, Recall: 0.8537, Precision: 0.9208\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.2675, Accuracy: 0.9004, F1: 0.8962, Recall: 0.9117, Precision: 0.8812\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.2780, Accuracy: 0.8961, F1: 0.8874, Recall: 0.8691, Precision: 0.9065\n",
      "Early stopping at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8914\n",
      "Test F1 Score: 0.8806\n",
      "Test Recall: 0.8603\n",
      "Test Precision: 0.9019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    df = pd.read_csv(\"C:\\\\Users\\\\USER\\\\Desktop\\\\충원's project\\\\IMCOM\\\\IMCOM_Edtech_apps(prepro+sentiment).csv\")\n",
    "    classifier = TransformerClassifier(df)\n",
    "    classifier.set_seed(42)\n",
    "    classifier.train()\n",
    "    classifier.evaluate()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "class TransformerClassifier:\n",
    "    def __init__(self, df, model_name=\"albert-base-v2\", max_length=64, batch_size=16, device=None):\n",
    "        self.df = df\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Tokenizer와 모델 초기화\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.config = AutoConfig.from_pretrained(self.model_name, num_labels=1)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, config=self.config)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def preprocess_data(self, df):\n",
    "        inputs = self.tokenizer(\n",
    "            list(df['prepro']),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = torch.tensor(df['label'].values)\n",
    "        return DataLoader(TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels), batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        # 저장된 가중치 로드\n",
    "        self.model.load_state_dict(torch.load(path, map_location=self.device))\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def evaluate(self):\n",
    "        df_train, df_temp = train_test_split(self.df, test_size=0.4, random_state=42)\n",
    "        df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
    "        test_loader = self.preprocess_data(df_test)\n",
    "\n",
    "        self.model.eval()  # 평가 모드로 전환\n",
    "        test_predictions_all = []\n",
    "        test_labels_all = []\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            for test_batch in tqdm(test_loader, desc=\"Test Batches\", leave=False):\n",
    "                input_ids, attention_mask, test_labels = [tensor.to(self.device) for tensor in test_batch]\n",
    "\n",
    "                test_outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                test_logits = test_outputs.logits.squeeze(dim=-1)\n",
    "\n",
    "                # Binary classification: use sigmoid and round for predictions\n",
    "                test_predictions = torch.round(torch.sigmoid(test_logits))\n",
    "\n",
    "                test_predictions_all.append(test_predictions.cpu())\n",
    "                test_labels_all.append(test_labels.cpu())\n",
    "\n",
    "        # 리스트들을 연결\n",
    "        test_predictions_all = torch.cat(test_predictions_all)\n",
    "        test_labels_all = torch.cat(test_labels_all)\n",
    "\n",
    "        # 평가 지표 계산\n",
    "        accuracy = accuracy_score(test_labels_all, test_predictions_all)\n",
    "        f1 = f1_score(test_labels_all, test_predictions_all)\n",
    "        recall = recall_score(test_labels_all, test_predictions_all)\n",
    "        precision = precision_score(test_labels_all, test_predictions_all)\n",
    "\n",
    "        # 평가 결과 출력\n",
    "        print(f'Test Accuracy: {accuracy:.4f}')\n",
    "        print(f'Test F1 Score: {f1:.4f}')\n",
    "        print(f'Test Recall: {recall:.4f}')\n",
    "        print(f'Test Precision: {precision:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_30288\\1839482032.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(path, map_location=self.device))\n",
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8914\n",
      "Test F1 Score: 0.8806\n",
      "Test Recall: 0.8603\n",
      "Test Precision: 0.9019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    df = pd.read_csv(\"C:\\\\Users\\\\USER\\\\Desktop\\\\충원's project\\\\IMCOM\\\\IMCOM_Edtech_apps(prepro+sentiment).csv\")\n",
    "    classifier = TransformerClassifier(df)\n",
    "    classifier.load_weights(\"best_model(ALBERT_sentiment2).pth\")  # 저장된 가중치 로드\n",
    "    classifier.evaluate()  # 로드된 가중치로 Test 데이터 평가\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
