{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":65260,"status":"ok","timestamp":1725956705320,"user":{"displayName":"강충원","userId":"17445879083622129016"},"user_tz":-540},"id":"lYy2ekxSLHVm","outputId":"86ddf852-8b49-4709-d012-50678f95938f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5783,"status":"ok","timestamp":1725956736577,"user":{"displayName":"강충원","userId":"17445879083622129016"},"user_tz":-540},"id":"Vgf77gwnMNA8","outputId":"8bb3b0ad-9a20-4ae9-eeb6-544ea9b082f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["import os\n","import torch\n","import random\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n","from torch.optim import AdamW\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AlbertConfig, AutoConfig\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch.nn.functional as F\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"]},{"cell_type":"markdown","metadata":{},"source":["## DeBERTa only"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":771,"referenced_widgets":["375b418146224a9f82162f1280ba24f3","6a56b64331394a41ab8b932ec282c600","d556f21a2afa43f089239d6d7b8f042a","6d3206f41b0849eda7f0d505f79ab230","96fcda50754546be868234d231d5ff7a","582af25ba6964ad28d2cfeba0b357c80","9fdc68c15a054268af8cb5abc65c22f3","cb86df177e0d4d4d8ee4c550dbeaf6d5","899f37f65ddc43cba6890423976e4c0d","bdb3f76b37574de3bbfdff283702b297","f8eabe90648144f0a7829780946af82c","40177744b4b64eecb74b0e37cc491b25","4a533ae7cff746e28865138802bc4801","1cac88a7d9644f9aad98111d200b25e6","81fcf672725d469a90e9b60572d50e91","f9ad0ce2fcee4e51a7b1e3c2793fbd97","8f17ae7b767442dc9aa7b2c12c25668e","35ce20782dee45809d3cab5edcaf401a","28e774052c4e4bb993d5a4afdebf6e35","d6b1370666634d2a9b89899f87ff61d6","09b33f351f724007acf83d21e846e5b5","49b964ff7a194b3ab687a8f1cb31b6ab","3bef536f6204494ab97a288696254277","d984eb3282114c37b25c6217fff887b6","9e26f549f52f46b0a291a60840222b12","850cd6c3e8c64cf49d4ed8ee1468b6df","7a7443449221458e9406a4bed88e4dea","91643cb28cc6449496282577bd0ecf83","bb3b2cf22bbd46238e37090124081e45","b90fd7aca1ad4b3fa6ae43059c9ad74b","56204d3d4eaf4607a3cfeb2107ba22ea","0e88b9fcd2d84d55bde0c762038c8f8f","f589e9ebd2634be186944842508cf63d","83a0085c10224a109ae0b6c7a93454e2","85695736f74240b6975f38c726ab4f83","920339c2f8614a7fa1705a363eb6c8d2","83be55b1eec94628adfa77febc19735f","31e5f627929447fea3bfc857fa285f2f","131d89dbfdbc4bfbaa238e279697e9df","e4be3f9a6ce84585bdd3fbc0b993c2a7","5fb3ea325ad04e10bc8d721b77ab9b2f","88b8b36c67764f3ea36609ae7eea5aad","045895326e4840089e344d9361f5dfd1","02b7292c0d5740df82d0ea23c7fb6171"]},"executionInfo":{"elapsed":1509893,"status":"ok","timestamp":1725592051779,"user":{"displayName":"강충원","userId":"17445879083622129016"},"user_tz":-540},"id":"JyAPdxmDMO2v","outputId":"015d54f2-9522-4a29-de00-06048de43c0e"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"375b418146224a9f82162f1280ba24f3","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"40177744b4b64eecb74b0e37cc491b25","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3bef536f6204494ab97a288696254277","version_major":2,"version_minor":0},"text/plain":["spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83a0085c10224a109ae0b6c7a93454e2","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 1/10\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["\n","Validation Loss: 0.2924, Accuracy: 0.9004, F1: 0.8942, Recall: 0.8930, Precision: 0.8955\n","\n","Epoch 2/10\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["\n","Validation Loss: 0.2594, Accuracy: 0.9074, F1: 0.9021, Recall: 0.9051, Precision: 0.8992\n","\n","Epoch 3/10\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["\n","Validation Loss: 0.2798, Accuracy: 0.9049, F1: 0.8964, Recall: 0.8723, Precision: 0.9218\n","\n","Epoch 4/10\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["\n","Validation Loss: 0.3049, Accuracy: 0.9053, F1: 0.8997, Recall: 0.9015, Precision: 0.8979\n","\n","Epoch 5/10\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["\n","Validation Loss: 0.2926, Accuracy: 0.9020, F1: 0.8982, Recall: 0.9177, Precision: 0.8796\n","Early stopping at epoch 5\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Test Accuracy: 0.8996\n","Test F1 Score: 0.8941\n","Test Recall: 0.9109\n","Test Precision: 0.8780\n"]}],"source":["class DeBERTa:\n","    def __init__(self, df, model_name=\"microsoft/deberta-v3-base\", max_length=64, learning_rate=2e-5, batch_size=16, epochs=10, patience=3, device=None):\n","        self.df = df\n","        self.model_name = model_name\n","        self.max_length = max_length\n","        self.learning_rate = learning_rate\n","        self.batch_size = batch_size\n","        self.epochs = epochs\n","        self.patience = patience\n","        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        # Tokenizer와 Config 불러오기\n","        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n","        self.config = AutoConfig.from_pretrained(self.model_name, hidden_dropout_prob=0.2, num_labels=1)  # num_labels=1로 설정\n","        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, config=self.config)\n","\n","        # 분류기 레이어를 이진 분류에 맞게 수정\n","        self.model.classifier = torch.nn.Linear(self.model.config.hidden_size, 1)\n","\n","        self.optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n","        self.best_model_state_dict = None\n","        self.best_accuracy = 0\n","\n","    @staticmethod\n","    def set_seed(seed):\n","        random.seed(seed)\n","        np.random.seed(seed)\n","        torch.manual_seed(seed)\n","        if torch.cuda.is_available():\n","            torch.cuda.manual_seed_all(seed)\n","\n","    def preprocess_data(self, df):\n","        inputs = self.tokenizer(\n","            list(df['prepro']),\n","            padding=True,\n","            truncation=True,\n","            max_length=self.max_length,\n","            return_tensors=\"pt\"\n","        )\n","        labels = torch.tensor(df['label'].values)\n","        return DataLoader(TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels), batch_size=self.batch_size, shuffle=True)\n","\n","    def train(self):\n","      df_train, df_temp = train_test_split(self.df, test_size=0.4, random_state=42)\n","      df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n","      train_loader = self.preprocess_data(df_train)\n","      val_loader = self.preprocess_data(df_val)\n","\n","      self.model.to(self.device)\n","\n","      patience_counter = 0\n","      min_val_loss = float('inf')\n","\n","      try:\n","          for epoch in range(self.epochs):\n","              print(f\"\\nEpoch {epoch + 1}/{self.epochs}\")\n","              self.model.train()\n","\n","              for input_batch in tqdm(train_loader, desc=\"Training Batches\", leave=False):\n","                  input_ids, attention_mask, label_batch = [tensor.to(self.device) for tensor in input_batch]\n","\n","                  self.optimizer.zero_grad()\n","                  outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n","\n","                  logits = outputs.logits[:, 0].squeeze(dim=-1)\n","                  loss = F.binary_cross_entropy_with_logits(logits, label_batch.float())\n","\n","                  loss.backward()\n","                  self.optimizer.step()\n","\n","              # Validation 단계\n","              self.model.eval()\n","              val_loss_total = 0\n","              val_predictions_all = []\n","              val_labels_all = []\n","\n","              with torch.inference_mode():\n","                  for val_batch in tqdm(val_loader, desc=\"Validation Batches\", leave=False):\n","                      input_ids, attention_mask, val_labels = [tensor.to(self.device) for tensor in val_batch]\n","\n","                      val_outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n","                      val_logits = val_outputs.logits[:, 0].squeeze(dim=-1)\n","\n","                      val_predictions = torch.round(torch.sigmoid(val_logits))\n","                      val_loss = F.binary_cross_entropy_with_logits(val_logits, val_labels.float())\n","                      val_loss_total += val_loss.item()\n","\n","                      val_predictions_all.append(val_predictions.cpu())\n","                      val_labels_all.append(val_labels.cpu())\n","\n","              val_predictions_all = torch.cat(val_predictions_all)\n","              val_labels_all = torch.cat(val_labels_all)\n","\n","              val_accuracy = accuracy_score(val_labels_all, val_predictions_all)\n","              val_f1 = f1_score(val_labels_all, val_predictions_all)\n","              val_recall = recall_score(val_labels_all, val_predictions_all)\n","              val_precision = precision_score(val_labels_all, val_predictions_all)\n","\n","              val_loss_total /= len(val_loader)\n","              print(f'\\nValidation Loss: {val_loss_total:.4f}, Accuracy: {val_accuracy:.4f}, F1: {val_f1:.4f}, Recall: {val_recall:.4f}, Precision: {val_precision:.4f}')\n","\n","              if val_loss_total < min_val_loss:\n","                  min_val_loss = val_loss_total\n","                  patience_counter = 0\n","                  self.best_model_state_dict = self.model.state_dict().copy()\n","              else:\n","                  patience_counter += 1\n","\n","              if patience_counter >= self.patience:\n","                print(f\"Early stopping at epoch {epoch + 1}\")\n","                # 가장 좋은 모델 상태 저장\n","                torch.save(self.best_model_state_dict, \"best_model(DeBERTa)_checkpoint.pth\")\n","                break\n","\n","      except Exception as e:\n","          print(f\"An error occurred during training: {str(e)}\")\n","          print(\"Saving current model weights...\")\n","          # 현재까지의 가중치를 저장\n","          torch.save(self.model.state_dict(), \"error_model(DeBERTa).pth\")\n","          raise  # 오류를 다시 발생시켜서 코드 실행 중단\n","\n","    def evaluate(self):\n","      if self.best_model_state_dict is None:\n","          raise ValueError(\"No trained model found. Please train the model first.\")\n","\n","      df_train, df_temp = train_test_split(self.df, test_size=0.4, random_state=42)\n","      df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n","      test_loader = self.preprocess_data(df_test)\n","\n","      # 가장 좋은 모델 가중치 로드\n","      best_model = AutoModelForSequenceClassification.from_pretrained(self.model_name, config=self.config)\n","      best_model.load_state_dict(self.best_model_state_dict, strict=False)  # strict=False로 설정하여 일부 키 불일치를 무시\n","      best_model.to(self.device)\n","\n","      best_model.eval()\n","      test_predictions_all = []\n","      test_labels_all = []\n","\n","      try:\n","          with torch.inference_mode():\n","              for test_batch in tqdm(test_loader, desc=\"Test Batches\", leave=False):\n","                  input_ids, attention_mask, test_labels = [tensor.to(self.device) for tensor in test_batch]\n","\n","                  test_outputs = best_model(input_ids=input_ids, attention_mask=attention_mask)\n","                  test_logits = test_outputs.logits[:, 0].squeeze(dim=-1)\n","\n","                  test_predictions = torch.round(torch.sigmoid(test_logits))\n","\n","                  test_predictions_all.append(test_predictions.cpu())\n","                  test_labels_all.append(test_labels.cpu())\n","\n","          test_predictions_all = torch.cat(test_predictions_all)\n","          test_labels_all = torch.cat(test_labels_all)\n","\n","          accuracy = accuracy_score(test_labels_all, test_predictions_all)\n","          f1 = f1_score(test_labels_all, test_predictions_all)\n","          recall = recall_score(test_labels_all, test_predictions_all)\n","          precision = precision_score(test_labels_all, test_predictions_all)\n","\n","          print(f'Test Accuracy: {accuracy:.4f}')\n","          print(f'Test F1 Score: {f1:.4f}')\n","          print(f'Test Recall: {recall:.4f}')\n","          print(f'Test Precision: {precision:.4f}')\n","\n","          # 최종 모델 저장\n","          torch.save(best_model.state_dict(), \"best_model(DeBERTa).pth\")\n","\n","      except Exception as e:\n","          print(f\"An error occurred during evaluation: {str(e)}\")\n","          print(\"Saving current model weights...\")\n","          torch.save(best_model.state_dict(), \"error_model(DeBERTa_evaluation).pth\")\n","          raise\n","\n","\n","def main():\n","    df = pd.read_csv(\"/content/drive/MyDrive/IMCOM_Edtech_apps(prepro+sentiment).csv\")\n","    classifier = DeBERTa(df)\n","    classifier.set_seed(42)\n","    classifier.train()\n","    classifier.evaluate()\n","\n","if __name__ == \"__main__\":\n","    main()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## DeBERTa + VADER"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":862,"referenced_widgets":["c0aefb22761a4590897ac54e9c943f16","db414b35fe704bc7b2477bc7c336e6f5","0ecc1d4d2224450d933e6d9c9444f827","06b54124613247078673640edba0304b","88a702213529443c8fdf730807495188","77807db0bb5d4ae3957bc43190ce6f9c","8bce29fc409c43c3bf0db1ac4b1da7db","4f5f6eae2c404d548e28fe9f1ea7dab4","f71b8d86308544289c6d16970ff75899","c2698885739a4aca91a76604f5e51972","c1ef12dcc9024038a84c2f0bdfa8b383","ab9252506066499c8d0290403890ca6a","0067b6a860324359ae51c7c359f7d574","9c7acb4443e8431db5e920d535fba322","495b13508c3d4d1ab67555b1ee746d34","795123fb198b4c68b7544e3a767e453a","2e4ad9b1e1ba46e7a973724349fa1489","37153a19cfba4d478e8969d589a71d82","6c485fa2582042b0a425d47a02a0b5d5","e01771967f124d768285587757581e0a","be6d39b2ff9b4ddda8ba41bef14d66d1","918974156234456fb413309f1935490c","65c10ea2723746c1810670657ee87ac9","db6a4aeb3e274c9fb49fa2aaecae166f","d9fd098b4f2c48cfa20530b7dc27f590","112537e555cb4966bb453abe6aabb2f9","3cd59e80fdb041e79401d512847ce80b","c39d1a865064411b9997d688b9b5f01b","d48b262be54e4779aa6030f222427fc3","324ca2c9a77f4c7485dc5a81751dee72","d0ba9650e02548ec9bd81f5e68257125","a113eff5512d4f1f84e02fd587a1b667","2da9aeb0812946a2aaf45fdc1efbeecc","e07d2ac80fc14d55a0e991a5b05bb528","cc89e97fea3b454cb1a4a2999244d895","7aec7549485c4ab8ac76317ad4f8c704","c386ef18e9514e2c9dedb83d1eeb6ae8","649c81fda00346a79544d1da5cb37723","397c98b440244df98c1f8fe85e6af76a","78350ca4ab0a4824b06b9c601c0c9df3","f2f2f34f8d1b4668824c76113c9025ee","3c56878b8b9a49fe911e3b2e2eba8dba","e7d7427799bd4f388ed65ba8e05f3c85","9d63c2cfc68c448e9256764be956d993"]},"executionInfo":{"elapsed":1451533,"status":"ok","timestamp":1725958276129,"user":{"displayName":"강충원","userId":"17445879083622129016"},"user_tz":-540},"id":"bZObyiiuB0NS","outputId":"0cb4b83a-4a49-4531-efa8-9166409031fc"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c0aefb22761a4590897ac54e9c943f16","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ab9252506066499c8d0290403890ca6a","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"65c10ea2723746c1810670657ee87ac9","version_major":2,"version_minor":0},"text/plain":["spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e07d2ac80fc14d55a0e991a5b05bb528","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 1/10\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["\n","Validation Loss: 0.2849, Accuracy: 0.9029, F1: 0.8977, Recall: 0.9040, Precision: 0.8916\n","\n","Epoch 2/10\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["\n","Validation Loss: 0.2607, Accuracy: 0.9039, F1: 0.8989, Recall: 0.9059, Precision: 0.8920\n","\n","Epoch 3/10\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["\n","Validation Loss: 0.2942, Accuracy: 0.9001, F1: 0.8899, Recall: 0.8567, Precision: 0.9257\n","\n","Epoch 4/10\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["\n","Validation Loss: 0.2910, Accuracy: 0.9033, F1: 0.8969, Recall: 0.8923, Precision: 0.9015\n","\n","Epoch 5/10\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["\n","Validation Loss: 0.3005, Accuracy: 0.9022, F1: 0.8976, Recall: 0.9089, Precision: 0.8865\n","Early stopping at epoch 5\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Test Accuracy: 0.9012\n","Test F1 Score: 0.8949\n","Test Recall: 0.9035\n","Test Precision: 0.8865\n"]}],"source":["class DeBERTa_vader:\n","    def __init__(self, df, model_name=\"microsoft/deberta-v3-base\", max_length=64, learning_rate=2e-5, batch_size=16, epochs=10, patience=3, device=None):\n","        self.df = df\n","        self.model_name = model_name\n","        self.max_length = max_length\n","        self.learning_rate = learning_rate\n","        self.batch_size = batch_size\n","        self.epochs = epochs\n","        self.patience = patience\n","        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        # Tokenizer와 Config 불러오기\n","        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n","        self.config = AutoConfig.from_pretrained(self.model_name, hidden_dropout_prob=0.2, num_labels=1)  # num_labels=1로 설정\n","        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, config=self.config)\n","\n","        # 분류기 레이어를 이진 분류에 맞게 수정\n","        self.model.classifier = torch.nn.Linear(self.model.config.hidden_size, 1)\n","\n","        self.optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n","        self.best_model_state_dict = None\n","        self.best_accuracy = 0\n","\n","    @staticmethod\n","    def set_seed(seed):\n","        random.seed(seed)\n","        np.random.seed(seed)\n","        torch.manual_seed(seed)\n","        if torch.cuda.is_available():\n","            torch.cuda.manual_seed_all(seed)\n","\n","    def preprocess_data(self, df):\n","        # 'prepro' 열과 'VD_senti' 열을 결합하여 하나의 입력으로 사용\n","        df['combined'] = df['prepro'] + ' ' + df['VD_senti'].astype(str)\n","\n","        # 결합된 텍스트를 사용하여 Tokenizer 적용\n","        inputs = self.tokenizer(\n","            list(df['combined']),\n","            padding=True,\n","            truncation=True,\n","            max_length=self.max_length,\n","            return_tensors=\"pt\"\n","        )\n","        labels = torch.tensor(df['label'].values)\n","        return DataLoader(TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels), batch_size=self.batch_size, shuffle=True)\n","\n","    def train(self):\n","        df_train, df_temp = train_test_split(self.df, test_size=0.4, random_state=42)\n","        df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n","        train_loader = self.preprocess_data(df_train)\n","        val_loader = self.preprocess_data(df_val)\n","\n","        self.model.to(self.device)\n","\n","        patience_counter = 0\n","        min_val_loss = float('inf')\n","\n","        try:\n","            for epoch in range(self.epochs):\n","                print(f\"\\nEpoch {epoch + 1}/{self.epochs}\")\n","                self.model.train()\n","\n","                for input_batch in tqdm(train_loader, desc=\"Training Batches\", leave=False):\n","                    input_ids, attention_mask, label_batch = [tensor.to(self.device) for tensor in input_batch]\n","\n","                    self.optimizer.zero_grad()\n","                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n","\n","                    logits = outputs.logits[:, 0].squeeze(dim=-1)\n","                    loss = F.binary_cross_entropy_with_logits(logits, label_batch.float())\n","\n","                    loss.backward()\n","                    self.optimizer.step()\n","\n","                # Validation 단계\n","                self.model.eval()\n","                val_loss_total = 0\n","                val_predictions_all = []\n","                val_labels_all = []\n","\n","                with torch.inference_mode():\n","                    for val_batch in tqdm(val_loader, desc=\"Validation Batches\", leave=False):\n","                        input_ids, attention_mask, val_labels = [tensor.to(self.device) for tensor in val_batch]\n","\n","                        val_outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n","                        val_logits = val_outputs.logits[:, 0].squeeze(dim=-1)\n","\n","                        val_predictions = torch.round(torch.sigmoid(val_logits))\n","                        val_loss = F.binary_cross_entropy_with_logits(val_logits, val_labels.float())\n","                        val_loss_total += val_loss.item()\n","\n","                        val_predictions_all.append(val_predictions.cpu())\n","                        val_labels_all.append(val_labels.cpu())\n","\n","                val_predictions_all = torch.cat(val_predictions_all)\n","                val_labels_all = torch.cat(val_labels_all)\n","\n","                val_accuracy = accuracy_score(val_labels_all, val_predictions_all)\n","                val_f1 = f1_score(val_labels_all, val_predictions_all)\n","                val_recall = recall_score(val_labels_all, val_predictions_all)\n","                val_precision = precision_score(val_labels_all, val_predictions_all)\n","\n","                val_loss_total /= len(val_loader)\n","                print(f'\\nValidation Loss: {val_loss_total:.4f}, Accuracy: {val_accuracy:.4f}, F1: {val_f1:.4f}, Recall: {val_recall:.4f}, Precision: {val_precision:.4f}')\n","\n","                if val_loss_total < min_val_loss:\n","                    min_val_loss = val_loss_total\n","                    patience_counter = 0\n","                    self.best_model_state_dict = self.model.state_dict().copy()\n","                else:\n","                    patience_counter += 1\n","\n","                if patience_counter >= self.patience:\n","                    print(f\"Early stopping at epoch {epoch + 1}\")\n","                    # 가장 좋은 모델 상태 저장\n","                    torch.save(self.best_model_state_dict, \"best_model(DeBERTavader)_checkpoint.pth\")\n","                    break\n","\n","        except Exception as e:\n","            print(f\"An error occurred during training: {str(e)}\")\n","            print(\"Saving current model weights...\")\n","            # 현재까지의 가중치를 저장\n","            torch.save(self.model.state_dict(), \"error_model(DeBERTavader).pth\")\n","            raise  # 오류를 다시 발생시켜서 코드 실행 중단\n","\n","    def evaluate(self):\n","        if self.best_model_state_dict is None:\n","            raise ValueError(\"No trained model found. Please train the model first.\")\n","\n","        df_train, df_temp = train_test_split(self.df, test_size=0.4, random_state=42)\n","        df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n","        test_loader = self.preprocess_data(df_test)\n","\n","        # 가장 좋은 모델 가중치 로드\n","        best_model = AutoModelForSequenceClassification.from_pretrained(self.model_name, config=self.config)\n","        best_model.load_state_dict(self.best_model_state_dict, strict=False)  # strict=False로 설정하여 일부 키 불일치를 무시\n","        best_model.to(self.device)\n","\n","        best_model.eval()\n","        test_predictions_all = []\n","        test_labels_all = []\n","\n","        try:\n","            with torch.inference_mode():\n","                for test_batch in tqdm(test_loader, desc=\"Test Batches\", leave=False):\n","                    input_ids, attention_mask, test_labels = [tensor.to(self.device) for tensor in test_batch]\n","\n","                    test_outputs = best_model(input_ids=input_ids, attention_mask=attention_mask)\n","                    test_logits = test_outputs.logits[:, 0].squeeze(dim=-1)\n","\n","                    test_predictions = torch.round(torch.sigmoid(test_logits))\n","\n","                    test_predictions_all.append(test_predictions.cpu())\n","                    test_labels_all.append(test_labels.cpu())\n","\n","            test_predictions_all = torch.cat(test_predictions_all)\n","            test_labels_all = torch.cat(test_labels_all)\n","\n","            accuracy = accuracy_score(test_labels_all, test_predictions_all)\n","            f1 = f1_score(test_labels_all, test_predictions_all)\n","            recall = recall_score(test_labels_all, test_predictions_all)\n","            precision = precision_score(test_labels_all, test_predictions_all)\n","\n","            print(f'Test Accuracy: {accuracy:.4f}')\n","            print(f'Test F1 Score: {f1:.4f}')\n","            print(f'Test Recall: {recall:.4f}')\n","            print(f'Test Precision: {precision:.4f}')\n","\n","            # 최종 모델 저장\n","            torch.save(best_model.state_dict(), \"best_model(DeBERTavader).pth\")\n","\n","        except Exception as e:\n","            print(f\"An error occurred during evaluation: {str(e)}\")\n","            print(\"Saving current model weights...\")\n","            torch.save(best_model.state_dict(), \"error_model(DeBERTavader_evaluation).pth\")\n","            raise\n","\n","\n","def main():\n","    df = pd.read_csv(\"/content/drive/MyDrive/IMCOM_Edtech_apps(prepro+sentiment).csv\")\n","    classifier = DeBERTa_vader(df)\n","    classifier.set_seed(42)\n","    classifier.train()\n","    classifier.evaluate()\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"markdown","metadata":{},"source":["## DeBERTa + TextBlob"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1474747,"status":"ok","timestamp":1725960055815,"user":{"displayName":"강충원","userId":"17445879083622129016"},"user_tz":-540},"id":"sTzOpKHFJFFV","outputId":"d60080e6-2c46-436c-f970-9137f52291e3"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 1/10\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["\n","Validation Loss: 0.2899, Accuracy: 0.8977, F1: 0.8946, Recall: 0.9203, Precision: 0.8702\n","\n","Epoch 2/10\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["\n","Validation Loss: 0.2614, Accuracy: 0.9026, F1: 0.8975, Recall: 0.9055, Precision: 0.8897\n","\n","Epoch 3/10\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["\n","Validation Loss: 0.2878, Accuracy: 0.9016, F1: 0.8914, Recall: 0.8575, Precision: 0.9281\n","\n","Epoch 4/10\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["\n","Validation Loss: 0.2854, Accuracy: 0.9063, F1: 0.9014, Recall: 0.9094, Precision: 0.8935\n","\n","Epoch 5/10\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["\n","Validation Loss: 0.3148, Accuracy: 0.9002, F1: 0.8970, Recall: 0.9222, Precision: 0.8731\n","Early stopping at epoch 5\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Test Accuracy: 0.8979\n","Test F1 Score: 0.8931\n","Test Recall: 0.9164\n","Test Precision: 0.8709\n"]}],"source":["class DeBERTa_tb:\n","    def __init__(self, df, model_name=\"microsoft/deberta-v3-base\", max_length=64, learning_rate=2e-5, batch_size=16, epochs=10, patience=3, device=None):\n","        self.df = df\n","        self.model_name = model_name\n","        self.max_length = max_length\n","        self.learning_rate = learning_rate\n","        self.batch_size = batch_size\n","        self.epochs = epochs\n","        self.patience = patience\n","        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        # Tokenizer와 Config 불러오기\n","        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n","        self.config = AutoConfig.from_pretrained(self.model_name, hidden_dropout_prob=0.2, num_labels=1)  # num_labels=1로 설정\n","        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, config=self.config)\n","\n","        # 분류기 레이어를 이진 분류에 맞게 수정\n","        self.model.classifier = torch.nn.Linear(self.model.config.hidden_size, 1)\n","\n","        self.optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n","        self.best_model_state_dict = None\n","        self.best_accuracy = 0\n","\n","    @staticmethod\n","    def set_seed(seed):\n","        random.seed(seed)\n","        np.random.seed(seed)\n","        torch.manual_seed(seed)\n","        if torch.cuda.is_available():\n","            torch.cuda.manual_seed_all(seed)\n","\n","    def preprocess_data(self, df):\n","        # 'prepro' 열과 'VD_senti' 열을 결합하여 하나의 입력으로 사용\n","        df['combined'] = df['prepro'] + ' ' + df['TB_senti'].astype(str)\n","\n","        # 결합된 텍스트를 사용하여 Tokenizer 적용\n","        inputs = self.tokenizer(\n","            list(df['combined']),\n","            padding=True,\n","            truncation=True,\n","            max_length=self.max_length,\n","            return_tensors=\"pt\"\n","        )\n","        labels = torch.tensor(df['label'].values)\n","        return DataLoader(TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels), batch_size=self.batch_size, shuffle=True)\n","\n","    def train(self):\n","        df_train, df_temp = train_test_split(self.df, test_size=0.4, random_state=42)\n","        df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n","        train_loader = self.preprocess_data(df_train)\n","        val_loader = self.preprocess_data(df_val)\n","\n","        self.model.to(self.device)\n","\n","        patience_counter = 0\n","        min_val_loss = float('inf')\n","\n","        try:\n","            for epoch in range(self.epochs):\n","                print(f\"\\nEpoch {epoch + 1}/{self.epochs}\")\n","                self.model.train()\n","\n","                for input_batch in tqdm(train_loader, desc=\"Training Batches\", leave=False):\n","                    input_ids, attention_mask, label_batch = [tensor.to(self.device) for tensor in input_batch]\n","\n","                    self.optimizer.zero_grad()\n","                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n","\n","                    logits = outputs.logits[:, 0].squeeze(dim=-1)\n","                    loss = F.binary_cross_entropy_with_logits(logits, label_batch.float())\n","\n","                    loss.backward()\n","                    self.optimizer.step()\n","\n","                # Validation 단계\n","                self.model.eval()\n","                val_loss_total = 0\n","                val_predictions_all = []\n","                val_labels_all = []\n","\n","                with torch.inference_mode():\n","                    for val_batch in tqdm(val_loader, desc=\"Validation Batches\", leave=False):\n","                        input_ids, attention_mask, val_labels = [tensor.to(self.device) for tensor in val_batch]\n","\n","                        val_outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n","                        val_logits = val_outputs.logits[:, 0].squeeze(dim=-1)\n","\n","                        val_predictions = torch.round(torch.sigmoid(val_logits))\n","                        val_loss = F.binary_cross_entropy_with_logits(val_logits, val_labels.float())\n","                        val_loss_total += val_loss.item()\n","\n","                        val_predictions_all.append(val_predictions.cpu())\n","                        val_labels_all.append(val_labels.cpu())\n","\n","                val_predictions_all = torch.cat(val_predictions_all)\n","                val_labels_all = torch.cat(val_labels_all)\n","\n","                val_accuracy = accuracy_score(val_labels_all, val_predictions_all)\n","                val_f1 = f1_score(val_labels_all, val_predictions_all)\n","                val_recall = recall_score(val_labels_all, val_predictions_all)\n","                val_precision = precision_score(val_labels_all, val_predictions_all)\n","\n","                val_loss_total /= len(val_loader)\n","                print(f'\\nValidation Loss: {val_loss_total:.4f}, Accuracy: {val_accuracy:.4f}, F1: {val_f1:.4f}, Recall: {val_recall:.4f}, Precision: {val_precision:.4f}')\n","\n","                if val_loss_total < min_val_loss:\n","                    min_val_loss = val_loss_total\n","                    patience_counter = 0\n","                    self.best_model_state_dict = self.model.state_dict().copy()\n","                else:\n","                    patience_counter += 1\n","\n","                if patience_counter >= self.patience:\n","                    print(f\"Early stopping at epoch {epoch + 1}\")\n","                    # 가장 좋은 모델 상태 저장\n","                    torch.save(self.best_model_state_dict, \"best_model(DeBERTatb)_checkpoint.pth\")\n","                    break\n","\n","        except Exception as e:\n","            print(f\"An error occurred during training: {str(e)}\")\n","            print(\"Saving current model weights...\")\n","            # 현재까지의 가중치를 저장\n","            torch.save(self.model.state_dict(), \"error_model(DeBERTatb).pth\")\n","            raise  # 오류를 다시 발생시켜서 코드 실행 중단\n","\n","    def evaluate(self):\n","        if self.best_model_state_dict is None:\n","            raise ValueError(\"No trained model found. Please train the model first.\")\n","\n","        df_train, df_temp = train_test_split(self.df, test_size=0.4, random_state=42)\n","        df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n","        test_loader = self.preprocess_data(df_test)\n","\n","        # 가장 좋은 모델 가중치 로드\n","        best_model = AutoModelForSequenceClassification.from_pretrained(self.model_name, config=self.config)\n","        best_model.load_state_dict(self.best_model_state_dict, strict=False)  # strict=False로 설정하여 일부 키 불일치를 무시\n","        best_model.to(self.device)\n","\n","        best_model.eval()\n","        test_predictions_all = []\n","        test_labels_all = []\n","\n","        try:\n","            with torch.inference_mode():\n","                for test_batch in tqdm(test_loader, desc=\"Test Batches\", leave=False):\n","                    input_ids, attention_mask, test_labels = [tensor.to(self.device) for tensor in test_batch]\n","\n","                    test_outputs = best_model(input_ids=input_ids, attention_mask=attention_mask)\n","                    test_logits = test_outputs.logits[:, 0].squeeze(dim=-1)\n","\n","                    test_predictions = torch.round(torch.sigmoid(test_logits))\n","\n","                    test_predictions_all.append(test_predictions.cpu())\n","                    test_labels_all.append(test_labels.cpu())\n","\n","            test_predictions_all = torch.cat(test_predictions_all)\n","            test_labels_all = torch.cat(test_labels_all)\n","\n","            accuracy = accuracy_score(test_labels_all, test_predictions_all)\n","            f1 = f1_score(test_labels_all, test_predictions_all)\n","            recall = recall_score(test_labels_all, test_predictions_all)\n","            precision = precision_score(test_labels_all, test_predictions_all)\n","\n","            print(f'Test Accuracy: {accuracy:.4f}')\n","            print(f'Test F1 Score: {f1:.4f}')\n","            print(f'Test Recall: {recall:.4f}')\n","            print(f'Test Precision: {precision:.4f}')\n","\n","            # 최종 모델 저장\n","            torch.save(best_model.state_dict(), \"best_model(DeBERTatb).pth\")\n","\n","        except Exception as e:\n","            print(f\"An error occurred during evaluation: {str(e)}\")\n","            print(\"Saving current model weights...\")\n","            torch.save(best_model.state_dict(), \"error_model(DeBERTatbr_evaluation).pth\")\n","            raise\n","\n","\n","def main():\n","    df = pd.read_csv(\"/content/drive/MyDrive/IMCOM_Edtech_apps(prepro+sentiment).csv\")\n","    classifier = DeBERTa_tb(df)\n","    classifier.set_seed(42)\n","    classifier.train()\n","    classifier.evaluate()\n","\n","if __name__ == \"__main__\":\n","    main()\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN9YAoCIMKXlSr5vhk928At","gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0067b6a860324359ae51c7c359f7d574":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e4ad9b1e1ba46e7a973724349fa1489","placeholder":"​","style":"IPY_MODEL_37153a19cfba4d478e8969d589a71d82","value":"config.json: 100%"}},"02b7292c0d5740df82d0ea23c7fb6171":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"045895326e4840089e344d9361f5dfd1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06b54124613247078673640edba0304b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2698885739a4aca91a76604f5e51972","placeholder":"​","style":"IPY_MODEL_c1ef12dcc9024038a84c2f0bdfa8b383","value":" 52.0/52.0 [00:00&lt;00:00, 3.94kB/s]"}},"09b33f351f724007acf83d21e846e5b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e88b9fcd2d84d55bde0c762038c8f8f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ecc1d4d2224450d933e6d9c9444f827":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f5f6eae2c404d548e28fe9f1ea7dab4","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f71b8d86308544289c6d16970ff75899","value":52}},"112537e555cb4966bb453abe6aabb2f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a113eff5512d4f1f84e02fd587a1b667","placeholder":"​","style":"IPY_MODEL_2da9aeb0812946a2aaf45fdc1efbeecc","value":" 2.46M/2.46M [00:00&lt;00:00, 40.4MB/s]"}},"131d89dbfdbc4bfbaa238e279697e9df":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cac88a7d9644f9aad98111d200b25e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_28e774052c4e4bb993d5a4afdebf6e35","max":579,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d6b1370666634d2a9b89899f87ff61d6","value":579}},"28e774052c4e4bb993d5a4afdebf6e35":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2da9aeb0812946a2aaf45fdc1efbeecc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e4ad9b1e1ba46e7a973724349fa1489":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31e5f627929447fea3bfc857fa285f2f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"324ca2c9a77f4c7485dc5a81751dee72":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35ce20782dee45809d3cab5edcaf401a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"37153a19cfba4d478e8969d589a71d82":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"375b418146224a9f82162f1280ba24f3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6a56b64331394a41ab8b932ec282c600","IPY_MODEL_d556f21a2afa43f089239d6d7b8f042a","IPY_MODEL_6d3206f41b0849eda7f0d505f79ab230"],"layout":"IPY_MODEL_96fcda50754546be868234d231d5ff7a"}},"397c98b440244df98c1f8fe85e6af76a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bef536f6204494ab97a288696254277":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d984eb3282114c37b25c6217fff887b6","IPY_MODEL_9e26f549f52f46b0a291a60840222b12","IPY_MODEL_850cd6c3e8c64cf49d4ed8ee1468b6df"],"layout":"IPY_MODEL_7a7443449221458e9406a4bed88e4dea"}},"3c56878b8b9a49fe911e3b2e2eba8dba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3cd59e80fdb041e79401d512847ce80b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40177744b4b64eecb74b0e37cc491b25":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4a533ae7cff746e28865138802bc4801","IPY_MODEL_1cac88a7d9644f9aad98111d200b25e6","IPY_MODEL_81fcf672725d469a90e9b60572d50e91"],"layout":"IPY_MODEL_f9ad0ce2fcee4e51a7b1e3c2793fbd97"}},"495b13508c3d4d1ab67555b1ee746d34":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be6d39b2ff9b4ddda8ba41bef14d66d1","placeholder":"​","style":"IPY_MODEL_918974156234456fb413309f1935490c","value":" 579/579 [00:00&lt;00:00, 45.9kB/s]"}},"49b964ff7a194b3ab687a8f1cb31b6ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a533ae7cff746e28865138802bc4801":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f17ae7b767442dc9aa7b2c12c25668e","placeholder":"​","style":"IPY_MODEL_35ce20782dee45809d3cab5edcaf401a","value":"config.json: 100%"}},"4f5f6eae2c404d548e28fe9f1ea7dab4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56204d3d4eaf4607a3cfeb2107ba22ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"582af25ba6964ad28d2cfeba0b357c80":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fb3ea325ad04e10bc8d721b77ab9b2f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"649c81fda00346a79544d1da5cb37723":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65c10ea2723746c1810670657ee87ac9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_db6a4aeb3e274c9fb49fa2aaecae166f","IPY_MODEL_d9fd098b4f2c48cfa20530b7dc27f590","IPY_MODEL_112537e555cb4966bb453abe6aabb2f9"],"layout":"IPY_MODEL_3cd59e80fdb041e79401d512847ce80b"}},"6a56b64331394a41ab8b932ec282c600":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_582af25ba6964ad28d2cfeba0b357c80","placeholder":"​","style":"IPY_MODEL_9fdc68c15a054268af8cb5abc65c22f3","value":"tokenizer_config.json: 100%"}},"6c485fa2582042b0a425d47a02a0b5d5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d3206f41b0849eda7f0d505f79ab230":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bdb3f76b37574de3bbfdff283702b297","placeholder":"​","style":"IPY_MODEL_f8eabe90648144f0a7829780946af82c","value":" 52.0/52.0 [00:00&lt;00:00, 4.39kB/s]"}},"77807db0bb5d4ae3957bc43190ce6f9c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78350ca4ab0a4824b06b9c601c0c9df3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"795123fb198b4c68b7544e3a767e453a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a7443449221458e9406a4bed88e4dea":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7aec7549485c4ab8ac76317ad4f8c704":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2f2f34f8d1b4668824c76113c9025ee","max":371146213,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3c56878b8b9a49fe911e3b2e2eba8dba","value":371146213}},"81fcf672725d469a90e9b60572d50e91":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_09b33f351f724007acf83d21e846e5b5","placeholder":"​","style":"IPY_MODEL_49b964ff7a194b3ab687a8f1cb31b6ab","value":" 579/579 [00:00&lt;00:00, 38.3kB/s]"}},"83a0085c10224a109ae0b6c7a93454e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_85695736f74240b6975f38c726ab4f83","IPY_MODEL_920339c2f8614a7fa1705a363eb6c8d2","IPY_MODEL_83be55b1eec94628adfa77febc19735f"],"layout":"IPY_MODEL_31e5f627929447fea3bfc857fa285f2f"}},"83be55b1eec94628adfa77febc19735f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_045895326e4840089e344d9361f5dfd1","placeholder":"​","style":"IPY_MODEL_02b7292c0d5740df82d0ea23c7fb6171","value":" 371M/371M [00:00&lt;00:00, 423MB/s]"}},"850cd6c3e8c64cf49d4ed8ee1468b6df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e88b9fcd2d84d55bde0c762038c8f8f","placeholder":"​","style":"IPY_MODEL_f589e9ebd2634be186944842508cf63d","value":" 2.46M/2.46M [00:00&lt;00:00, 129MB/s]"}},"85695736f74240b6975f38c726ab4f83":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_131d89dbfdbc4bfbaa238e279697e9df","placeholder":"​","style":"IPY_MODEL_e4be3f9a6ce84585bdd3fbc0b993c2a7","value":"pytorch_model.bin: 100%"}},"88a702213529443c8fdf730807495188":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88b8b36c67764f3ea36609ae7eea5aad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"899f37f65ddc43cba6890423976e4c0d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8bce29fc409c43c3bf0db1ac4b1da7db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f17ae7b767442dc9aa7b2c12c25668e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91643cb28cc6449496282577bd0ecf83":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"918974156234456fb413309f1935490c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"920339c2f8614a7fa1705a363eb6c8d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5fb3ea325ad04e10bc8d721b77ab9b2f","max":371146213,"min":0,"orientation":"horizontal","style":"IPY_MODEL_88b8b36c67764f3ea36609ae7eea5aad","value":371146213}},"96fcda50754546be868234d231d5ff7a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c7acb4443e8431db5e920d535fba322":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c485fa2582042b0a425d47a02a0b5d5","max":579,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e01771967f124d768285587757581e0a","value":579}},"9d63c2cfc68c448e9256764be956d993":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e26f549f52f46b0a291a60840222b12":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b90fd7aca1ad4b3fa6ae43059c9ad74b","max":2464616,"min":0,"orientation":"horizontal","style":"IPY_MODEL_56204d3d4eaf4607a3cfeb2107ba22ea","value":2464616}},"9fdc68c15a054268af8cb5abc65c22f3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a113eff5512d4f1f84e02fd587a1b667":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab9252506066499c8d0290403890ca6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0067b6a860324359ae51c7c359f7d574","IPY_MODEL_9c7acb4443e8431db5e920d535fba322","IPY_MODEL_495b13508c3d4d1ab67555b1ee746d34"],"layout":"IPY_MODEL_795123fb198b4c68b7544e3a767e453a"}},"b90fd7aca1ad4b3fa6ae43059c9ad74b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb3b2cf22bbd46238e37090124081e45":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bdb3f76b37574de3bbfdff283702b297":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be6d39b2ff9b4ddda8ba41bef14d66d1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0aefb22761a4590897ac54e9c943f16":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_db414b35fe704bc7b2477bc7c336e6f5","IPY_MODEL_0ecc1d4d2224450d933e6d9c9444f827","IPY_MODEL_06b54124613247078673640edba0304b"],"layout":"IPY_MODEL_88a702213529443c8fdf730807495188"}},"c1ef12dcc9024038a84c2f0bdfa8b383":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c2698885739a4aca91a76604f5e51972":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c386ef18e9514e2c9dedb83d1eeb6ae8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7d7427799bd4f388ed65ba8e05f3c85","placeholder":"​","style":"IPY_MODEL_9d63c2cfc68c448e9256764be956d993","value":" 371M/371M [00:02&lt;00:00, 185MB/s]"}},"c39d1a865064411b9997d688b9b5f01b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb86df177e0d4d4d8ee4c550dbeaf6d5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc89e97fea3b454cb1a4a2999244d895":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_397c98b440244df98c1f8fe85e6af76a","placeholder":"​","style":"IPY_MODEL_78350ca4ab0a4824b06b9c601c0c9df3","value":"pytorch_model.bin: 100%"}},"d0ba9650e02548ec9bd81f5e68257125":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d48b262be54e4779aa6030f222427fc3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d556f21a2afa43f089239d6d7b8f042a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb86df177e0d4d4d8ee4c550dbeaf6d5","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_899f37f65ddc43cba6890423976e4c0d","value":52}},"d6b1370666634d2a9b89899f87ff61d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d984eb3282114c37b25c6217fff887b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91643cb28cc6449496282577bd0ecf83","placeholder":"​","style":"IPY_MODEL_bb3b2cf22bbd46238e37090124081e45","value":"spm.model: 100%"}},"d9fd098b4f2c48cfa20530b7dc27f590":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_324ca2c9a77f4c7485dc5a81751dee72","max":2464616,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d0ba9650e02548ec9bd81f5e68257125","value":2464616}},"db414b35fe704bc7b2477bc7c336e6f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77807db0bb5d4ae3957bc43190ce6f9c","placeholder":"​","style":"IPY_MODEL_8bce29fc409c43c3bf0db1ac4b1da7db","value":"tokenizer_config.json: 100%"}},"db6a4aeb3e274c9fb49fa2aaecae166f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c39d1a865064411b9997d688b9b5f01b","placeholder":"​","style":"IPY_MODEL_d48b262be54e4779aa6030f222427fc3","value":"spm.model: 100%"}},"e01771967f124d768285587757581e0a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e07d2ac80fc14d55a0e991a5b05bb528":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cc89e97fea3b454cb1a4a2999244d895","IPY_MODEL_7aec7549485c4ab8ac76317ad4f8c704","IPY_MODEL_c386ef18e9514e2c9dedb83d1eeb6ae8"],"layout":"IPY_MODEL_649c81fda00346a79544d1da5cb37723"}},"e4be3f9a6ce84585bdd3fbc0b993c2a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e7d7427799bd4f388ed65ba8e05f3c85":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2f2f34f8d1b4668824c76113c9025ee":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f589e9ebd2634be186944842508cf63d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f71b8d86308544289c6d16970ff75899":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f8eabe90648144f0a7829780946af82c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f9ad0ce2fcee4e51a7b1e3c2793fbd97":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
